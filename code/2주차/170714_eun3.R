## SVM -> 회귀와 분류쪽에서 성능이 뛰어남
# 전체 데이터 사용하지 않고, 데이터 일부만 사용함
# 하나의 선에서 거리(margin(C))가 1만큼 떨어진 선 2개, 2개의 선에 있는 점(POINT)들을 SUPPORT VECTOR(SV)라고 함
# 선 상에 딱 떨어지는 변수가 없으면 근방에 있는 변수(여유변수 또는 슬랙변수)
# 여러개의 선이 나올 수 있고 각 선마다 정분류율을 계산하여 가장 높은 분류율의 선을 선택!
# 중요포인트를 찾고 이것을 통해 의미있는 분류선(결정선 또는 초평면)을 찾는 것임

# w, b, 슬랙변수들을 원변수라고 함
# 비선형을 선형을 만들어주는 함수 : 커널함수 -> 어떤 값을 넘겨줄지에 따라 쓰이는 커널함수가 '다름'
# (ex. 1or0을 넘겨줄 것이라면 ->sigmoid)


## 신경망
# y = a + b1x1 + b2x2 + b3x3 (a는 상수항, b는 가중치, x는 변수) -> 변환함수(f) -> Y.hat가 구해짐
# -> 알고있는 Y값-구한 값(Y-Y.hat)이 최소가 되는 방향으로 반복
# 변수가 많을 때 중간에 필터링을 한번 함, 이것을 hidden layer
# hidden layer가 여러 개 있는 것을 '딥러닝'이라 표현함
# Y-Y.hat의 최소값을 찾아가면서 최적의 가중치(w)를 업데이트시키는 것

# 변수들이 전방향으로 움직일 수 있으려면 우선 w에 대한 '초기값'이 존재해야 함